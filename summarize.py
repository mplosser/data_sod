"""
Summarize Summary of Deposits parquet files.

This script scans all SOD parquet files and generates a summary showing:
- Coverage by year
- Number of branches per year
- Number of variables per year
- File sizes
- Overall statistics

Usage:
    # Summarize with default parallelization
    python summarize_raw_data.py --input-dir ../../data/sod/raw_annual

    # Save summary to CSV
    python summarize_raw_data.py \\
        --input-dir ../../data/sod/raw_annual \\
        --output-csv sod_summary.csv

    # Disable parallelization
    python summarize_raw_data.py \\
        --input-dir ../../data/sod/raw_annual \\
        --no-parallel
"""

import pandas as pd
import argparse
import sys
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing


def analyze_file(file_path_str):
    """
    Analyze a single parquet file.

    Args:
        file_path_str: Path to parquet file as string

    Returns:
        Dictionary with file info or None if error
    """
    file_path = Path(file_path_str)

    try:
        # Read parquet file
        df = pd.read_parquet(file_path)

        # Extract year from filename (e.g., "2020.parquet" -> 2020)
        year = int(file_path.stem)

        # Get reporting period
        if 'REPORTING_PERIOD' in df.columns:
            reporting_period = df['REPORTING_PERIOD'].iloc[0]
        else:
            # Fallback to June 30 of year
            reporting_period = pd.Timestamp(year=year, month=6, day=30)

        # Get file size
        file_size_mb = file_path.stat().st_size / (1024 * 1024)

        return {
            'year': year,
            'date': reporting_period,
            'branches': len(df),
            'variables': len(df.columns) - 2,  # Exclude CERT and REPORTING_PERIOD
            'total_columns': len(df.columns),
            'size_mb': file_size_mb,
            'file': file_path.name
        }

    except Exception as e:
        print(f"Error processing {file_path.name}: {e}")
        return None


def main():
    parser = argparse.ArgumentParser(
        description='Summarize SOD parquet files',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Generate summary
  python summarize_raw_data.py --input-dir ../../data/sod/raw_annual

  # Save to CSV
  python summarize_raw_data.py \\
      --input-dir ../../data/sod/raw_annual \\
      --output-csv sod_summary.csv

  # Disable parallelization (for low-memory systems)
  python summarize_raw_data.py \\
      --input-dir ../../data/sod/raw_annual \\
      --no-parallel
        """
    )

    parser.add_argument(
        '--input-dir',
        type=str,
        required=True,
        help='Directory containing parquet files'
    )

    parser.add_argument(
        '--workers',
        type=int,
        default=None,
        help='Number of parallel workers (default: all CPUs)'
    )

    parser.add_argument(
        '--no-parallel',
        action='store_true',
        help='Disable parallel processing'
    )

    parser.add_argument(
        '--output-csv',
        type=str,
        help='Save summary to CSV file'
    )

    args = parser.parse_args()

    # Setup
    input_dir = Path(args.input_dir)

    if not input_dir.exists():
        print(f"ERROR: Directory does not exist: {input_dir}")
        sys.exit(1)

    # Find parquet files
    parquet_files = sorted(input_dir.glob('*.parquet'))

    if not parquet_files:
        print(f"No parquet files found in {input_dir}")
        return 1

    # Determine worker count
    if args.no_parallel:
        workers = 1
    elif args.workers:
        workers = args.workers
    else:
        workers = multiprocessing.cpu_count()

    print("="*80)
    print("SUMMARY OF DEPOSITS DATA SUMMARY")
    print("="*80)
    print(f"Directory: {input_dir}")
    print(f"Files found: {len(parquet_files)}")
    print(f"Parallel workers: {workers}")
    print("="*80)

    # Analyze files
    results = []

    if workers == 1:
        # Sequential processing
        print("\nAnalyzing files sequentially...")
        for file_path in parquet_files:
            result = analyze_file(str(file_path))
            if result:
                results.append(result)
                print(f"  Processed {result['year']}")

    else:
        # Parallel processing
        print(f"\nProcessing files in parallel with {workers} workers...")

        with ProcessPoolExecutor(max_workers=workers) as executor:
            future_to_file = {
                executor.submit(analyze_file, str(f)): f
                for f in parquet_files
            }

            completed = 0
            for future in as_completed(future_to_file):
                completed += 1

                try:
                    result = future.result()
                    if result:
                        results.append(result)

                    # Progress update
                    if completed % 10 == 0 or completed == len(parquet_files):
                        print(f"  Processed {completed}/{len(parquet_files)} files...")

                except Exception as e:
                    print(f"  Error: {e}")

    if not results:
        print("\nNo valid data found")
        return 1

    # Create summary DataFrame
    df_summary = pd.DataFrame(results)
    df_summary = df_summary.sort_values('year')

    # Print summary table
    print()
    print(f"{'Year':<6} {'Date':<12} {'Branches':>9} {'Variables':>10} {'Size (MB)':>10}")
    print("-" * 6 + " " + "-" * 12 + " " + "-" * 9 + " " + "-" * 10 + " " + "-" * 10)

    for _, row in df_summary.iterrows():
        print(f"{row['year']:<6} {row['date'].strftime('%Y-%m-%d'):<12} "
              f"{row['branches']:>9,} {row['variables']:>10,} {row['size_mb']:>10.1f}")

    # Overall statistics
    print("\n" + "="*80)
    print("OVERALL STATISTICS")
    print("="*80)
    print(f"Total years: {len(df_summary)}")
    print(f"Date range: {df_summary['date'].min().strftime('%Y-%m-%d')} to {df_summary['date'].max().strftime('%Y-%m-%d')}")
    print(f"Branches (avg): {df_summary['branches'].mean():,.0f}")
    print(f"Branches (min): {df_summary['branches'].min():,}")
    print(f"Branches (max): {df_summary['branches'].max():,}")
    print(f"Variables (avg): {df_summary['variables'].mean():.0f}")
    print(f"Variables (min): {df_summary['variables'].min()}")
    print(f"Variables (max): {df_summary['variables'].max()}")
    print(f"Total size: {df_summary['size_mb'].sum():.1f} MB")
    print("="*80)

    # Save to CSV if requested
    if args.output_csv:
        df_summary.to_csv(args.output_csv, index=False)
        print(f"\nSummary saved to: {args.output_csv}")

    return 0


if __name__ == '__main__':
    sys.exit(main())
